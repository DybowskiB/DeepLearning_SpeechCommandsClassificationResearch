{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "171989a5",
   "metadata": {},
   "source": [
    "# Speech Commands Classification with Transformer\n",
    "\n",
    "Author: Jakub Borek, Bartosz Dybowski\n",
    "\n",
    "Model with pre-trained model Wav2Vec."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1debc5fb-2052-4399-ae0c-358d65337ba0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "from transformers import Wav2Vec2Processor, Wav2Vec2Model\n",
    "import torch\n",
    "import os\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "from concurrent.futures import ThreadPoolExecutor\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d805c5f6-e601-40de-9ef7-e9c659033edc",
   "metadata": {},
   "outputs": [],
   "source": [
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d0a6835-ab80-4438-9494-22fb3d4f0ebd",
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN_AUDIO_DIR = './data/train/train/audio'\n",
    "TEST_AUDIO_DIR = './data/test/test/audio'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c6ea796-bfee-4e63-ac2e-7da29cdad7a1",
   "metadata": {},
   "outputs": [],
   "source": [
    "processor = Wav2Vec2Processor.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model = Wav2Vec2Model.from_pretrained(\"facebook/wav2vec2-base-960h\")\n",
    "model.eval()\n",
    "\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c030d599-5fe3-4683-8e24-cb6a5311ab11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_wav_16k_mono(path):\n",
    "    wav, sr = librosa.load(path, sr=16000)\n",
    "    return wav\n",
    "\n",
    "def parallel_load_audio(file_paths, max_workers=8):\n",
    "    with ThreadPoolExecutor(max_workers=max_workers) as executor:\n",
    "        audios = list(tqdm(executor.map(load_wav_16k_mono, file_paths), total=len(file_paths), desc=\"Loading Audio Files\"))\n",
    "    return audios\n",
    "\n",
    "def extract_wav2vec_embeddings(file_paths, batch_size=4, num_workers=8):\n",
    "    embeddings = []\n",
    "\n",
    "    audios = parallel_load_audio(file_paths, max_workers=num_workers)\n",
    "    \n",
    "    for i in tqdm(range(0, len(audios), batch_size), desc=\"Extracting Embeddings\"):\n",
    "        batch = audios[i:i+batch_size]\n",
    "        \n",
    "        inputs = processor(batch, return_tensors=\"pt\", padding=True, sampling_rate=16000).input_values.to(device)\n",
    "        \n",
    "        with torch.no_grad():\n",
    "            outputs = model(inputs).last_hidden_state  # shape (batch_size, time, features)\n",
    "        \n",
    "        pooled = outputs.mean(dim=1).cpu().numpy()\n",
    "        embeddings.extend(pooled)\n",
    "    \n",
    "    embeddings = np.vstack(embeddings)\n",
    "    return embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ef97da0-0354-456b-9fae-f6b18a43e0c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_label_names(audio_dir):\n",
    "    all_labels = sorted(os.listdir(audio_dir))\n",
    "    label_names = [label for label in all_labels if not label.startswith('_')]\n",
    "    return label_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a60d1f05-9514-4f48-8e9b-a4d8393a992a",
   "metadata": {},
   "outputs": [],
   "source": [
    "label_names_list = get_label_names(TRAIN_AUDIO_DIR)\n",
    "print(label_names_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "943dd961-67f5-4e7e-a4db-78296e47e732",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_file_paths_and_labels(audio_dir, max_per_class=20000):\n",
    "    file_paths, labels = [], []\n",
    "    label_names = label_names_list\n",
    "    label_to_index = {label: idx for idx, label in enumerate(label_names)}\n",
    "\n",
    "    for label in label_names:\n",
    "        folder = os.path.join(audio_dir, label)\n",
    "        if not os.path.isdir(folder):\n",
    "            continue\n",
    "        for i, fname in enumerate(os.listdir(folder)):\n",
    "            if fname.endswith(\".wav\") and i < max_per_class:\n",
    "                path = os.path.join(folder, fname)\n",
    "                file_paths.append(path)\n",
    "                labels.append(label_to_index[label])\n",
    "    return file_paths, np.array(labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8af83e9-7dd7-4ea6-bfbc-39ad4b4cd376",
   "metadata": {},
   "outputs": [],
   "source": [
    "file_paths, labels = load_file_paths_and_labels(TRAIN_AUDIO_DIR)\n",
    "print(f\"Total files: {len(file_paths)}\")\n",
    "print(f\"Labels array shape: {labels.shape}\")\n",
    "\n",
    "X_train_paths, X_temp_paths, y_train, y_temp = train_test_split(\n",
    "    file_paths, labels, test_size=0.3, random_state=42, stratify=labels\n",
    ")\n",
    "\n",
    "X_val_paths, X_test_paths, y_val, y_test = train_test_split(\n",
    "    X_temp_paths, y_temp, test_size=0.5, random_state=42, stratify=y_temp\n",
    ")\n",
    "\n",
    "print(f\"Train set: {len(X_train_paths)} files\")\n",
    "print(f\"Val set: {len(X_val_paths)} files\")\n",
    "print(f\"Test set: {len(X_test_paths)} files\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2de0ccc-ea26-4bdf-b9bf-5b8cd3dc11e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_embed = extract_wav2vec_embeddings(X_train_paths)\n",
    "X_val_embed = extract_wav2vec_embeddings(X_val_paths)\n",
    "X_test_embed = extract_wav2vec_embeddings(X_test_paths)\n",
    "\n",
    "X_train_tensor = torch.tensor(X_train_embed, dtype=torch.float32)\n",
    "y_train_tensor = torch.tensor(y_train, dtype=torch.long)\n",
    "\n",
    "X_val_tensor = torch.tensor(X_val_embed, dtype=torch.float32)\n",
    "y_val_tensor = torch.tensor(y_val, dtype=torch.long)\n",
    "\n",
    "X_test_tensor = torch.tensor(X_test_embed, dtype=torch.float32)\n",
    "y_test_tensor = torch.tensor(y_test, dtype=torch.long)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c594386d-d650-462d-8ca8-1c8b6ae72ccd",
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 64\n",
    "\n",
    "train_loader = DataLoader(TensorDataset(X_train_tensor, y_train_tensor), batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(TensorDataset(X_val_tensor, y_val_tensor), batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(TensorDataset(X_test_tensor, y_test_tensor), batch_size=batch_size, shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9258cc9c-0a81-4d4f-9631-eda9e542fab3",
   "metadata": {},
   "outputs": [],
   "source": [
    "class MLPClassifier(nn.Module):\n",
    "    def __init__(self, input_dim=768, hidden_dim=256, num_classes=len(np.unique(y_train))):\n",
    "        super(MLPClassifier, self).__init__()\n",
    "        self.net = nn.Sequential(\n",
    "            nn.Linear(input_dim, hidden_dim),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(hidden_dim, 128),\n",
    "            nn.ReLU(),\n",
    "            nn.Dropout(0.2),\n",
    "            nn.Linear(128, num_classes)\n",
    "        )\n",
    "    \n",
    "    def forward(self, x):\n",
    "        return self.net(x)\n",
    "\n",
    "model_clf = MLPClassifier()\n",
    "model_clf.to(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6834e6fd-4878-4b0e-9170-9b2b901ccf84",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_losses = []\n",
    "val_losses = []\n",
    "train_accuracies = []\n",
    "val_accuracies = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c38975-41d8-4324-856b-091790373935",
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model_clf.parameters(), lr=1e-5)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "n_epochs = 50\n",
    "\n",
    "for epoch in range(n_epochs):\n",
    "    model_clf.train()\n",
    "    train_loss, correct = 0, 0\n",
    "\n",
    "    for xb, yb in train_loader:\n",
    "        xb, yb = xb.to(device), yb.to(device)\n",
    "        optimizer.zero_grad()\n",
    "        preds = model_clf(xb)\n",
    "        loss = criterion(preds, yb)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        train_loss += loss.item()\n",
    "        correct += (preds.argmax(1) == yb).sum().item()\n",
    "\n",
    "    train_acc = correct / len(train_loader.dataset)\n",
    "\n",
    "    model_clf.eval()\n",
    "    val_loss, val_correct = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for xb, yb in val_loader:\n",
    "            xb, yb = xb.to(device), yb.to(device)\n",
    "            preds = model_clf(xb)\n",
    "            loss = criterion(preds, yb)\n",
    "            val_loss += loss.item()\n",
    "            val_correct += (preds.argmax(1) == yb).sum().item()\n",
    "\n",
    "    val_acc = val_correct / len(val_loader.dataset)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    train_accuracies.append(train_acc)\n",
    "    val_accuracies.append(val_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: \"\n",
    "          f\"Train Loss={train_loss:.3f}, Train Acc={train_acc:.4f} | \"\n",
    "          f\"Val Loss={val_loss:.3f}, Val Acc={val_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97a954cd-291a-458f-acee-1a9cd3f4a317",
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.figure(figsize=(12, 5))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Loss over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(train_accuracies, label='Train Accuracy')\n",
    "plt.plot(val_accuracies, label='Validation Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy over Epochs')\n",
    "plt.legend()\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85cc0fcb-bdfb-4415-9c64-78a17377cba8",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_clf.eval()\n",
    "with torch.no_grad():\n",
    "    outputs = model_clf(X_test_tensor.to(device))\n",
    "    y_pred_test = outputs.argmax(dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3a27e64-11a6-46c3-a94c-9cdf4b0b3f4c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "test_acc = accuracy_score(y_test, y_pred_test)\n",
    "print(f\"Test Accuracy: {test_acc:.4f}\")\n",
    "\n",
    "cm = confusion_matrix(y_test, y_pred_test)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=label_names_list)\n",
    "\n",
    "correct_per_class = cm.diagonal()\n",
    "total_per_class = cm.sum(axis=1)\n",
    "acc_per_class = correct_per_class / total_per_class\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "disp.plot(ax=ax, cmap='Blues', values_format='d')\n",
    "\n",
    "plt.title(\"Confusion Matrix on Test Set\")\n",
    "plt.xticks(rotation=45, fontsize=8)  \n",
    "plt.yticks(fontsize=8)              \n",
    "plt.show()\n",
    "\n",
    "\n",
    "for label, acc in zip(label_names_list, acc_per_class):\n",
    "    print(f\"Class '{label}': Accuracy = {acc:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9589d35c-4694-4aea-b823-c79fc38a5204",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
