{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f4af91e6",
   "metadata": {
    "id": "f4af91e6"
   },
   "source": [
    "# Speech Commands Classification with ResNet\n",
    "\n",
    "Author: Jakub Borek, Bartosz Dybowski\n",
    "\n",
    "Model with pre-trained model ResNet-18."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "46bdb5be",
   "metadata": {
    "id": "46bdb5be"
   },
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397b0a81",
   "metadata": {
    "id": "397b0a81",
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install soundfile\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "import torchvision.models as models\n",
    "import torch.nn.functional as F\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import random\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import os"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e616395d",
   "metadata": {
    "id": "e616395d"
   },
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7fc8673",
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "d7fc8673",
    "outputId": "d8a3f675-ca04-4b72-a8d8-c4c8c91e1f34"
   },
   "outputs": [],
   "source": [
    "use_all_classes = 2  # 0 for use all classes, 1 for use yes/no, 2 for use silence/unknown 3 for use 11 classes\n",
    "batch_size = 256\n",
    "learning_rate = 0.001\n",
    "epochs = 50\n",
    "\n",
    "## Device\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "print(\"Available GPUs:\", torch.cuda.device_count())\n",
    "print(\"Current device:\", torch.cuda.current_device())\n",
    "print(\"Device name:\", torch.cuda.get_device_name(torch.cuda.current_device()))\n",
    "\n",
    "def set_seed(seed):\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "\n",
    "# Call seed function\n",
    "set_seed(1)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c83a0212",
   "metadata": {
    "id": "c83a0212"
   },
   "source": [
    "## Prepare data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "babdc96d",
   "metadata": {
    "id": "babdc96d"
   },
   "outputs": [],
   "source": [
    "class SubsetSC(torchaudio.datasets.SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(root=\"./SpeechCommands\", download=True)\n",
    "\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as f:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in f]\n",
    "\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "\n",
    "# MelSpectrogram transform\n",
    "transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=64)\n",
    "\n",
    "# Prepare labels depending on the mode\n",
    "if use_all_classes == 0:\n",
    "    labels = sorted(list(set(datapoint[2] for datapoint in SubsetSC(\"training\"))))\n",
    "elif use_all_classes == 1:\n",
    "    labels = ['yes', 'no']\n",
    "elif use_all_classes == 2:\n",
    "    labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence']\n",
    "elif use_all_classes == 3:\n",
    "    labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown']\n",
    "\n",
    "label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Load background noise files\n",
    "background_noises = []\n",
    "if use_all_classes == 2:\n",
    "    background_dir = \"./SpeechCommands/SpeechCommands/speech_commands_v0.02/_background_noise_\"\n",
    "    if os.path.exists(background_dir):\n",
    "        print(\"Found noises\")\n",
    "        for filename in os.listdir(background_dir):\n",
    "            if filename.endswith('.wav'):\n",
    "                path = os.path.join(background_dir, filename)\n",
    "                waveform, sr = torchaudio.load(path)\n",
    "                background_noises.append(waveform.squeeze(0))\n",
    "\n",
    "# Custom collate function\n",
    "def collate_fn(batch, silence_probability=0.9):\n",
    "    tensors = []\n",
    "    targets = []\n",
    "    max_len = 128\n",
    "    silence_duration_samples = 16000  # (1s = 16000 samples)\n",
    "\n",
    "    for waveform, sample_rate, label, *_ in batch:\n",
    "        if use_all_classes == 1:\n",
    "            if label not in ['yes', 'no']:\n",
    "                label = 'unknown'\n",
    "        elif use_all_classes == 2:\n",
    "            if label not in ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']:\n",
    "                label = 'unknown'\n",
    "        elif use_all_classes == 3:\n",
    "            if label not in ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go']:\n",
    "                label = 'unknown'\n",
    "\n",
    "        if label not in labels:\n",
    "            continue\n",
    "\n",
    "        spec = transform(waveform).squeeze(0)\n",
    "\n",
    "        if spec.shape[-1] > max_len:\n",
    "            spec = spec[:, :max_len]\n",
    "        elif spec.shape[-1] < max_len:\n",
    "            pad_size = max_len - spec.shape[-1]\n",
    "            spec = torch.nn.functional.pad(spec, (0, pad_size))\n",
    "\n",
    "        tensors.append(spec)\n",
    "        targets.append(label_to_index[label])\n",
    "\n",
    "    # Inject background noise as silence\n",
    "    if use_all_classes == 2 and background_noises:\n",
    "        num_silence = int(len(tensors) * silence_probability)\n",
    "        for _ in range(num_silence):\n",
    "            noise = random.choice(background_noises)\n",
    "            if noise.size(0) >= silence_duration_samples:\n",
    "                start = random.randint(0, noise.size(0) - silence_duration_samples)\n",
    "                silence_waveform = noise[start:start + silence_duration_samples]\n",
    "            else:\n",
    "                silence_waveform = torch.nn.functional.pad(noise, (0, silence_duration_samples - noise.size(0)))\n",
    "\n",
    "            silence_spec = transform(silence_waveform.unsqueeze(0)).squeeze(0)\n",
    "            if silence_spec.shape[-1] > max_len:\n",
    "                silence_spec = silence_spec[:, :max_len]\n",
    "            elif silence_spec.shape[-1] < max_len:\n",
    "                pad_size = max_len - silence_spec.shape[-1]\n",
    "                silence_spec = torch.nn.functional.pad(silence_spec, (0, pad_size))\n",
    "\n",
    "            tensors.append(silence_spec)\n",
    "            targets.append(label_to_index['silence'])\n",
    "\n",
    "    if len(tensors) == 0:\n",
    "        return torch.empty(0), torch.empty(0, dtype=torch.long)\n",
    "\n",
    "    tensors = torch.stack(tensors)\n",
    "    targets = torch.tensor(targets)\n",
    "    return tensors, targets\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "Qt7VD4miySCe",
   "metadata": {
    "id": "Qt7VD4miySCe"
   },
   "source": [
    "## Load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "vT-u5YGiyTim",
   "metadata": {
    "id": "vT-u5YGiyTim"
   },
   "outputs": [],
   "source": [
    "train_set = SubsetSC(\"training\")\n",
    "val_set = SubsetSC(\"validation\")\n",
    "test_set = SubsetSC(\"testing\")\n",
    "\n",
    "train_loader = DataLoader(train_set, batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(val_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(test_set, batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=8, pin_memory=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0652e89",
   "metadata": {
    "id": "e0652e89"
   },
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff070691",
   "metadata": {
    "id": "ff070691"
   },
   "outputs": [],
   "source": [
    "class AudioResNet(nn.Module):\n",
    "    def __init__(self, num_classes):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7, 7), stride=(2, 2), padding=(3, 3), bias=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, num_classes)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "model = AudioResNet(num_classes=len(labels)).to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aee6a408",
   "metadata": {
    "id": "aee6a408"
   },
   "source": [
    "## Model features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1e93663",
   "metadata": {
    "id": "a1e93663"
   },
   "outputs": [],
   "source": [
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ad9b825c",
   "metadata": {
    "id": "ad9b825c"
   },
   "source": [
    "## Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "49438493",
   "metadata": {
    "id": "49438493"
   },
   "outputs": [],
   "source": [
    "print(\"Training...\")\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    running_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    for inputs, targets in loader:\n",
    "        if inputs.size(0) == 0:\n",
    "            continue\n",
    "        \n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "    return running_loss / len(loader), correct / total\n",
    "\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_targets = []\n",
    "    all_preds = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets in loader:\n",
    "            if inputs.numel() == 0:\n",
    "                continue  # Skip empty batch\n",
    "            \n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "\n",
    "            all_targets.extend(targets.cpu().numpy())\n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "\n",
    "    if total == 0:\n",
    "        return 0, [], []\n",
    "\n",
    "    accuracy = correct / total\n",
    "    return accuracy, all_targets, all_preds\n",
    "\n",
    "train_losses, val_losses, test_losses = [], [], []\n",
    "train_accs, val_accs, test_accs = [], [], []\n",
    "\n",
    "best_val_acc = 0.0\n",
    "best_epoch = 0\n",
    "\n",
    "for epoch in range(epochs):\n",
    "    train_loss, train_acc = train(model, train_loader)\n",
    "    val_acc, _, _ = evaluate(model, val_loader)\n",
    "    test_acc, _, _ = evaluate(model, test_loader)\n",
    "\n",
    "    train_losses.append(train_loss)\n",
    "    train_accs.append(train_acc)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    if val_acc > best_val_acc:\n",
    "      best_val_acc = val_acc\n",
    "      best_epoch = epoch + 1\n",
    "\n",
    "    print(f\"Epoch {epoch+1}: Train loss {train_loss:.4f}, Train acc {train_acc:.4f}, Val acc {val_acc:.4f}, Test acc {test_acc:.4f}\")\n",
    "\n",
    "    # Save model after each epoch\n",
    "    torch.save(model.state_dict(), f\"model_epoch_{epoch+1}.pt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1648dc1c-92fe-4248-ad66-f805eea9abb3",
   "metadata": {
    "id": "d9896c6c"
   },
   "source": [
    "## Load the best model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd7bf049-1383-4d0e-abbe-80357d9feb06",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading best model from epoch {best_epoch} with val acc {best_val_acc:.4f}\")\n",
    "model.load_state_dict(torch.load(f\"model_epoch_{best_epoch}.pt\"))\n",
    "model = model.to(device)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9896c6c",
   "metadata": {
    "id": "d9896c6c"
   },
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5335867",
   "metadata": {
    "editable": true,
    "id": "d5335867",
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "train_acc, all_targets, all_preds = evaluate(model, train_loader)\n",
    "print(f\"Train accuracy: {test_acc:.4f}\")\n",
    "\n",
    "val_acc_final, val_targets, val_preds = evaluate(model, val_loader)\n",
    "print(f\"Validation accuracy: {val_acc_final:.4f}\")\n",
    "\n",
    "test_acc, all_targets, all_preds = evaluate(model, test_loader)\n",
    "print(f\"Test accuracy: {test_acc:.4f}\")\n",
    "\n",
    "# Loss plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, epochs+1), train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"train_loss.jpg\")\n",
    "plt.show()\n",
    "\n",
    "# Accuracy plot\n",
    "plt.figure(figsize=(10,6))\n",
    "plt.plot(range(1, epochs+1), train_accs, label='Train Accuracy')\n",
    "plt.plot(range(1, epochs+1), val_accs, label='Validation Accuracy')\n",
    "plt.plot(range(1, epochs+1), test_accs, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Accuracy per Epoch')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.savefig(\"accuracy_per_epoch.jpg\")\n",
    "plt.show()\n",
    "\n",
    "# Generate confusion matrix\n",
    "cm = confusion_matrix(all_targets, all_preds)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=labels)\n",
    "fig, ax = plt.subplots(figsize=(12, 12))\n",
    "fig.patch.set_facecolor('white')\n",
    "ax.set_facecolor('white')\n",
    "disp.plot(\n",
    "    ax=ax,\n",
    "    cmap='Blues',              \n",
    "    colorbar=False,            \n",
    "    values_format='d'         \n",
    ")\n",
    "plt.title(\"Confusion Matrix\", fontsize=20)\n",
    "plt.xlabel('Predicted label', fontsize=16)\n",
    "plt.ylabel('True label', fontsize=16)\n",
    "plt.xticks(rotation=45, fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.grid(False)\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"confusion_matrix.jpg\")\n",
    "plt.show()\n",
    "\n",
    "# Easiest and hardest commands (Precision per class)\n",
    "from sklearn.metrics import precision_score\n",
    "\n",
    "precisions = precision_score(all_targets, all_preds, average=None)\n",
    "for idx, label in enumerate(labels):\n",
    "    print(f\"Precision for class '{label}': {precisions[idx]:.4f}\")\n",
    "\n",
    "# Analysis of recording length (whether shorter recordings are easier to classify)\n",
    "\n",
    "waveform_lengths = [waveform.shape[-1] for waveform, *_ in test_set]\n",
    "short_indices = [i for i, l in enumerate(waveform_lengths) if l < 16000]  # less than 1 second\n",
    "long_indices = [i for i, l in enumerate(waveform_lengths) if l >= 16000]\n",
    "\n",
    "short_correct = sum(1 for i in short_indices if all_preds[i] == all_targets[i])\n",
    "long_correct = sum(1 for i in long_indices if all_preds[i] == all_targets[i])\n",
    "\n",
    "print(f\"Short samples accuracy: {short_correct / len(short_indices):.4f}\")\n",
    "print(f\"Long samples accuracy: {long_correct / len(long_indices):.4f}\")\n",
    "\n",
    "# Impact of loudness (mean amplitude of the signal)\n",
    "\n",
    "amplitudes = [torch.abs(waveform).mean().item() for waveform, *_ in test_set]\n",
    "high_amp_indices = [i for i, a in enumerate(amplitudes) if a > 0.05]\n",
    "low_amp_indices = [i for i, a in enumerate(amplitudes) if a <= 0.05]\n",
    "\n",
    "high_amp_correct = sum(1 for i in high_amp_indices if all_preds[i] == all_targets[i])\n",
    "low_amp_correct = sum(1 for i in low_amp_indices if all_preds[i] == all_targets[i])\n",
    "\n",
    "print(f\"High amplitude samples accuracy: {high_amp_correct / len(high_amp_indices):.4f}\")\n",
    "print(f\"Low amplitude samples accuracy: {low_amp_correct / len(low_amp_indices):.4f}\")\n",
    "\n",
    "# Pure analysis of \"unknown\" and \"silence\" classes\n",
    "\n",
    "unknown_idx = [i for i, t in enumerate(all_targets) if labels[t] == 'unknown'] if 'unknown' in labels else []\n",
    "silence_idx = [i for i, t in enumerate(all_targets) if labels[t] == 'silence'] if 'silence' in labels else []\n",
    "\n",
    "if unknown_idx:\n",
    "    unknown_correct = sum(1 for i in unknown_idx if all_preds[i] == all_targets[i])\n",
    "    print(f\"Unknown class accuracy: {unknown_correct / len(unknown_idx):.4f}\")\n",
    "if silence_idx:\n",
    "    silence_correct = sum(1 for i in silence_idx if all_preds[i] == all_targets[i])\n",
    "    print(f\"Silence class accuracy: {silence_correct / len(silence_idx):.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "931e1c64-e9f5-467f-9d8a-772e3b59b932",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "machine_shape": "hm",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
