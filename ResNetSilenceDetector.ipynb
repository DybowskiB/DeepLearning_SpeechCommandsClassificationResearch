{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c4653e3d",
   "metadata": {},
   "source": [
    "# Speech Commands Classification with ResNet (detecting silence)\n",
    "\n",
    "Author: Jakub Borek, Bartosz Dybowski\n",
    "\n",
    "Model with pre-trained model ResNet-18."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44ea4236",
   "metadata": {},
   "source": [
    "## Import"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54db8b7b",
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install matplotlib\n",
    "!pip install scikit-learn\n",
    "!pip install soundfile\n",
    "\n",
    "import os\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torchaudio\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import models\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "add3688f",
   "metadata": {},
   "source": [
    "## Settings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56dcd98c",
   "metadata": {},
   "outputs": [],
   "source": [
    "use_all_classes = 2\n",
    "batch_size = 256\n",
    "learning_rate = 1e-3\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "epochs = 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c422904",
   "metadata": {},
   "source": [
    "## Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d36dfd0c",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SubsetSC(torchaudio.datasets.SPEECHCOMMANDS):\n",
    "    def __init__(self, subset: str = None):\n",
    "        super().__init__(root=\"./SpeechCommands\", download=True)\n",
    "        def load_list(filename):\n",
    "            filepath = os.path.join(self._path, filename)\n",
    "            with open(filepath) as f:\n",
    "                return [os.path.normpath(os.path.join(self._path, line.strip())) for line in f]\n",
    "        if subset == \"validation\":\n",
    "            self._walker = load_list(\"validation_list.txt\")\n",
    "        elif subset == \"testing\":\n",
    "            self._walker = load_list(\"testing_list.txt\")\n",
    "        elif subset == \"training\":\n",
    "            excludes = load_list(\"validation_list.txt\") + load_list(\"testing_list.txt\")\n",
    "            excludes = set(excludes)\n",
    "            self._walker = [w for w in self._walker if w not in excludes]\n",
    "\n",
    "transform = torchaudio.transforms.MelSpectrogram(sample_rate=16000, n_mels=64)\n",
    "\n",
    "labels = ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go', 'unknown', 'silence']\n",
    "label_to_index = {label: i for i, label in enumerate(labels)}\n",
    "\n",
    "# Background noise\n",
    "background_noises = []\n",
    "background_dir = \"./SpeechCommands/SpeechCommands/speech_commands_v0.02/_background_noise_\"\n",
    "if os.path.exists(background_dir):\n",
    "    print(\"Found background noises.\")\n",
    "    for filename in os.listdir(background_dir):\n",
    "        if filename.endswith('.wav'):\n",
    "            path = os.path.join(background_dir, filename)\n",
    "            waveform, sr = torchaudio.load(path)\n",
    "            background_noises.append(waveform.squeeze(0))\n",
    "\n",
    "# Collate function\n",
    "def collate_fn(batch, silence_probability=0.1):\n",
    "    tensors, targets_silence, _ = [], [], []\n",
    "    max_len = 128\n",
    "    silence_duration_samples = 16000\n",
    "\n",
    "    for waveform, sample_rate, label, *_ in batch:\n",
    "        if label not in labels:\n",
    "            continue\n",
    "        silence_label = 1 if label in ['yes', 'no', 'up', 'down', 'left', 'right', 'on', 'off', 'stop', 'go'] else 0\n",
    "        spec = transform(waveform).squeeze(0)\n",
    "        if spec.shape[-1] > max_len:\n",
    "            spec = spec[:, :max_len]\n",
    "        elif spec.shape[-1] < max_len:\n",
    "            spec = torch.nn.functional.pad(spec, (0, max_len - spec.shape[-1]))\n",
    "        tensors.append(spec)\n",
    "        targets_silence.append(silence_label)\n",
    "\n",
    "    if background_noises:\n",
    "        num_silence = int(len(tensors) * silence_probability)\n",
    "        for _ in range(num_silence):\n",
    "            noise = random.choice(background_noises)\n",
    "            if noise.size(0) >= silence_duration_samples:\n",
    "                start = random.randint(0, noise.size(0) - silence_duration_samples)\n",
    "                silence_waveform = noise[start:start + silence_duration_samples]\n",
    "            else:\n",
    "                silence_waveform = torch.nn.functional.pad(noise, (0, silence_duration_samples - noise.size(0)))\n",
    "            silence_spec = transform(silence_waveform.unsqueeze(0)).squeeze(0)\n",
    "            if silence_spec.shape[-1] > max_len:\n",
    "                silence_spec = silence_spec[:, :max_len]\n",
    "            elif silence_spec.shape[-1] < max_len:\n",
    "                silence_spec = torch.nn.functional.pad(silence_spec, (0, max_len - silence_spec.shape[-1]))\n",
    "            tensors.append(silence_spec)\n",
    "            targets_silence.append(0)\n",
    "\n",
    "    if len(tensors) == 0:\n",
    "        return torch.empty(0), torch.empty(0), torch.empty(0)\n",
    "\n",
    "    return torch.stack(tensors), torch.tensor(targets_silence), torch.empty(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d515368",
   "metadata": {},
   "source": [
    "## Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52bb4a65",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SilenceDetector(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.resnet = models.resnet18(pretrained=True)\n",
    "        self.resnet.conv1 = nn.Conv2d(1, 64, kernel_size=(7,7), stride=(2,2), padding=(3,3), bias=False)\n",
    "        self.resnet.fc = nn.Linear(self.resnet.fc.in_features, 2)\n",
    "    def forward(self, x):\n",
    "        return self.resnet(x)\n",
    "\n",
    "# DataLoader\n",
    "train_loader = DataLoader(SubsetSC(\"training\"), batch_size=batch_size, shuffle=True, collate_fn=collate_fn, num_workers=8, pin_memory=True)\n",
    "val_loader = DataLoader(SubsetSC(\"validation\"), batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=8, pin_memory=True)\n",
    "test_loader = DataLoader(SubsetSC(\"testing\"), batch_size=batch_size, shuffle=False, collate_fn=collate_fn, num_workers=8, pin_memory=True)\n",
    "\n",
    "# Model, criterion, optimizer\n",
    "silence_detector = SilenceDetector().to(device)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(silence_detector.parameters(), lr=learning_rate)\n",
    "\n",
    "# Trenowanie\n",
    "train_losses, train_accs = [], []\n",
    "val_accs, test_accs = [], []\n",
    "\n",
    "def evaluate(model, loader):\n",
    "    model.eval()\n",
    "    correct, total = 0, 0\n",
    "    with torch.no_grad():\n",
    "        for inputs, targets, _ in loader:\n",
    "            if inputs.numel() == 0:\n",
    "                continue\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            inputs = inputs.unsqueeze(1)\n",
    "            outputs = model(inputs)\n",
    "            _, predicted = torch.max(outputs, 1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "    return correct / total\n",
    "\n",
    "def train(model, loader):\n",
    "    model.train()\n",
    "    running_loss, correct, total = 0, 0, 0\n",
    "    for inputs, targets, _ in loader:\n",
    "        if inputs.numel() == 0:\n",
    "            continue\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        optimizer.zero_grad()\n",
    "        outputs = model(inputs)\n",
    "        loss = criterion(outputs, targets)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        running_loss += loss.item()\n",
    "        _, predicted = torch.max(outputs, 1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "    train_losses.append(running_loss / len(loader))\n",
    "    train_accs.append(correct / total)\n",
    "\n",
    "# Main training loop\n",
    "for epoch in range(epochs):\n",
    "    train(silence_detector, train_loader)\n",
    "    val_acc = evaluate(silence_detector, val_loader)\n",
    "    test_acc = evaluate(silence_detector, test_loader)\n",
    "    val_accs.append(val_acc)\n",
    "    test_accs.append(test_acc)\n",
    "\n",
    "    print(f\"Epoch {epoch+1}/{epochs}: Train Loss {train_losses[-1]:.4f}, Train Acc {train_accs[-1]:.4f}, Val Acc {val_acc:.4f}, Test Acc {test_acc:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "196af275",
   "metadata": {},
   "source": [
    "## Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f0d73e6-b03b-41f7-81a7-57b7f61f8a7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Wykres Loss\n",
    "plt.figure()\n",
    "plt.plot(train_losses, label='Train Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training Loss')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Wykres Accuracy\n",
    "plt.figure()\n",
    "plt.plot(train_accs, label='Train Accuracy')\n",
    "plt.plot(val_accs, label='Validation Accuracy')\n",
    "plt.plot(test_accs, label='Test Accuracy')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Accuracy')\n",
    "plt.title('Training/Validation/Test Accuracy')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "# Confusion Matrix\n",
    "silence_detector.eval()\n",
    "all_preds, all_labels = [], []\n",
    "with torch.no_grad():\n",
    "    for inputs, targets, _ in test_loader:\n",
    "        if inputs.numel() == 0:\n",
    "            continue\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        inputs = inputs.unsqueeze(1)\n",
    "        outputs = silence_detector(inputs)\n",
    "        _, preds = torch.max(outputs, 1)\n",
    "        all_preds.extend(preds.cpu().numpy())\n",
    "        all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "cm = confusion_matrix(all_labels, all_preds)\n",
    "ConfusionMatrixDisplay(cm, display_labels=['Non-speech', 'Speech']).plot()\n",
    "plt.title('Confusion Matrix - Silence Detector')\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
